# README ‚Äì Trabajos Pr√°cticos de Procesamiento de Lenguaje Natural 2

Este repositorio contiene tres trabajos pr√°cticos que exploran diferentes t√©cnicas y arquitecturas para modelos generativos, chatbots y sistema de agente.

üìå Trabajo 1 ‚Äî An√°lisis de Estrategias de Decodificaci√≥n con TinyGPT y MoE
Descripci√≥n

En este trabajo se estudi√≥ la generaci√≥n de secuencias con un modelo base (TinyGPT) y una versi√≥n mejorada con arquitectura Mixture of Experts (MoE).

Objetivos

Evaluar c√≥mo afectan diferentes t√©cnicas de muestreo y b√∫squeda en la calidad del texto generado.

Comparar el rendimiento entre un modelo tradicional (TinyGPT) y una variante mejorada con MoE.

Identificar limitaciones en la generaci√≥n de lenguaje con tokenizaci√≥n por caracteres.

T√©cnicas de Decodificaci√≥n Analizadas

Greedy Search ‚Üí Genera resultados predecibles, pero con riesgo de repeticiones.

Muestreo Aleatorio ‚Üí Mayor diversidad, pero sacrifica coherencia.

Muestreo con Temperatura ‚Üí Controla la variabilidad de las respuestas.

Top-K Sampling ‚Üí Restringe la generaci√≥n a los k tokens m√°s probables.

Top-P (Nucleus) Sampling ‚Üí Ajusta din√°micamente el rango de tokens considerados.

Resultados

Existe un trade-off entre diversidad y coherencia:

Mayor aleatoriedad ‚áí m√°s creatividad, menos sentido.

Mayor determinismo ‚áí coherencia estable, riesgo de repeticiones.

La tokenizaci√≥n por caracteres limita fuertemente la calidad gramatical.

El modelo MoE logra mayor variabilidad en las predicciones, incluso usando m√©todos deterministas como greedy.

El uso de m√∫ltiples expertos sugiere que, con una mejor tokenizaci√≥n, se podr√≠an obtener mejoras a√∫n m√°s significativas.


üìå Trabajo 2 ‚Äî Implementaci√≥n de Chatbot con RAG (Retrieval-Augmented Generation)
Descripci√≥n

En este trabajo se implement√≥ un chatbot inteligente capaz de recuperar informaci√≥n de documentos y generar respuestas m√°s completas utilizando la t√©cnica de Retrieval-Augmented Generation (RAG).

La informaci√≥n de los CVs se preprocesa y se convierte en embeddings mediante Sentence-Transformers. Estos embeddings se almacenan en √≠ndices vectoriales de Pinecone, que permiten realizar b√∫squedas sem√°nticas r√°pidas y precisas.

Cuando el usuario realiza una consulta, el chatbot:

Convierte la pregunta en un embedding.

Busca en Pinecone los fragmentos m√°s relevantes del CV.

Usa ese contexto recuperado para generar una respuesta enriquecida y coherente.

Objetivos

Combinar b√∫squeda sem√°ntica con generaci√≥n de texto para mejorar la precisi√≥n de las respuestas.

Utilizar Pinecone como vector store para almacenar y recuperar informaci√≥n de manera eficiente.

Desarrollar una interfaz interactiva con Streamlit para facilitar las consultas.



üìå Trabajo 3 ‚Äî Chatbot con Nodo Decisor para Consulta de M√∫ltiples CVs
Descripci√≥n

Este trabajo es una extensi√≥n del Trabajo 2, en la que se implement√≥ un chatbot mejorado que puede responder consultas sobre diferentes CVs utilizando un √∫nico agente.

La clave de esta implementaci√≥n es la incorporaci√≥n de un nodo decisor, que analiza la consulta del usuario y determina qu√© √≠ndice de Pinecone debe consultar para recuperar el contexto adecuado. Una vez que se obtiene la informaci√≥n relevante, el mismo agente genera la respuesta final.

Funcionamiento

An√°lisis de la consulta ‚Üí El nodo decisor detecta si en la pregunta se menciona un nombre espec√≠fico.

Selecci√≥n del √≠ndice ‚Üí

Si se menciona a una persona ‚Üí Se consulta el √≠ndice correspondiente en Pinecone.

Si no se menciona a nadie ‚Üí Por defecto, se consulta el √≠ndice del alumno.

Recuperaci√≥n de contexto ‚Üí Se buscan los fragmentos m√°s relevantes del CV desde el √≠ndice seleccionado.

Generaci√≥n de la respuesta ‚Üí El agente utiliza el contexto recuperado para construir una respuesta clara y precisa.

Objetivos

Extender la funcionalidad del chatbot para responder consultas sobre m√∫ltiples CVs.

Implementar un nodo decisor que seleccione el √≠ndice correcto en Pinecone seg√∫n la consulta.

Mantener un √∫nico agente para simplificar la arquitectura, pero con acceso a m√∫ltiples fuentes de informaci√≥n.

